{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) dcinside.com\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "    \n",
    "import re\n",
    "import time, datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os, sys, glob\n",
    "\n",
    "pages = 6935\n",
    "dc_us_url = \"https://gall.dcinside.com/mgallery/board/lists/?id=stockus&page=\"\n",
    "dc_us2_url = \"https://gall.dcinside.com/mgallery/board/lists/?id=tenbagger&page=\"\n",
    "dc_ko_url = \"https://gall.dcinside.com/mgallery/board/lists/?id=kospi&page=\"\n",
    "dc_dow_url = \"https://gall.dcinside.com/mgallery/board/lists/?id=dow100&page=\"\n",
    "\n",
    "def dc_crawling(name, base_url, pages):\n",
    "    path = \"./temp\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    nrows = 50\n",
    "    error_pages = []\n",
    "    for page in tqdm(range(1, pages + 1)):\n",
    "        url = base_url + str(page)\n",
    "        try:\n",
    "            res = requests.get(url, headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        except:\n",
    "            print(f\"Error: {page} page\\n\")\n",
    "            error_pages.append(page)\n",
    "\n",
    "        data_temp = []\n",
    "        try:\n",
    "            for idx in range(nrows):\n",
    "                i_content = soup.find_all(\"tr\", class_=\"ub-content us-post\")[idx]\n",
    "                date = i_content.find(\"td\", class_=\"gall_date\")[\"title\"][:10]\n",
    "                title = i_content.find(\"a\").text\n",
    "                view = i_content.find(\"td\", class_=\"gall_count\").text\n",
    "                like = i_content.find(\"td\", class_=\"gall_recommend\").text\n",
    "                reply = i_content.find(\"span\",class_=\"reply_num\")\n",
    "                if reply == None:\n",
    "                    reply = \"0\"\n",
    "                else:\n",
    "                    reply = i_content.find(\"span\",class_=\"reply_num\").text.strip(\"[,]\")\n",
    "                data_temp.append([date, title,view,like,reply])\n",
    "        except:\n",
    "            print(f\"Out of Index Error: {page} page\\n\")\n",
    "        \n",
    "        df = pd.DataFrame(data_temp, columns=[\"Date\",\n",
    "                                              \"Title\",\n",
    "                                              \"View\",\n",
    "                                              \"Like\",\n",
    "                                              \"Reply\"])\n",
    "\n",
    "        df.to_csv(f\"./temp/{str(page).zfill(len(str(pages)))}.csv\", encoding=\"utf-8\")\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    # 분산파일 통합\n",
    "    file_path = glob.glob(path+\"/*.csv\")\n",
    "\n",
    "    data_temp = []\n",
    "    for f in tqdm(file_path):\n",
    "        temp = os.path.basename(f).split(\".\")[0]\n",
    "        data_temp.extend(pd.read_csv(f, index_col=0).values.tolist())\n",
    "    df = pd.DataFrame(data_temp, columns=[\"Date\"\n",
    "                                        ,\"Title\"\n",
    "                                        ,\"View\"\n",
    "                                        ,\"Like\"\n",
    "                                        ,\"Reply\"])\n",
    "    df.to_csv(f\"./{name}.csv\", encoding=\"utf-8\")\n",
    "    \n",
    "    # 분산파일 제거\n",
    "    for f in file_path:\n",
    "        os.remove(f)\n",
    "\n",
    "    return df\n",
    "\n",
    "dc_df = dc_crawling(\"dc_nasdaq_test\", dc_us_url, pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) investing.com crawling\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "import time, datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os, sys, glob\n",
    "\n",
    "pages = 500\n",
    "\n",
    "nq_100_url = \"https://kr.investing.com/indices/nq-100-futures-commentary/\"\n",
    "nq_url = \"https://kr.investing.com/indices/nasdaq-composite-commentary/\"\n",
    "ko_url = \"https://kr.investing.com/indices/kospi-commentary/\"\n",
    "\n",
    "def investing_crawling(name, url, pages):\n",
    "    ################################################################################################\n",
    "    def date_reshape(time, now):\n",
    "        if re.findall(r\"[가-힣]\", time)[0] == \"분\":\n",
    "            t = int(re.sub(r\"[가-힣\\s]\", \"\", time))\n",
    "            reshaped_time = (now - datetime.timedelta(minutes=t)).strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        elif re.findall(r\"[가-힣]\",time)[0] == \"시\":\n",
    "            t = int(re.sub(r\"[가-힣\\s]\", \"\", time))\n",
    "            reshaped_time = (now - datetime.timedelta(hours=t)).strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "        elif re.findall(r\"[가-힣]\",time)[0] == \"방\":\n",
    "            return now.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        else:\n",
    "            year = time.split()[0][:-1]\n",
    "            month = time.split()[1][:-1]\n",
    "            day = time.split()[2][:-1]\n",
    "            reshaped_time = datetime.datetime.strptime(year + \"-\" + month + \"-\" + day,\"%Y-%m-%d\")\n",
    "        return reshaped_time\n",
    "    ################################################################################################\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    error_pages = []\n",
    "    for page in tqdm(range(1,pages+1)):\n",
    "        try:\n",
    "            url = url + str(page) \n",
    "            req = urllib.request.Request(url, headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
    "            f = urllib.request.urlopen(req).read().decode(\"utf-8\")\n",
    "            soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "            nrows = len(soup.find_all('div', class_=\"comment_content__AvzPV\"))\n",
    "            data_temp = []\n",
    "            \n",
    "            for idx in range(nrows):\n",
    "                comment = soup.find_all('div', class_=\"comment_content__AvzPV\")[idx].text\n",
    "                temp_time = soup.find_all('span', class_=\"comment_user-info__AWjKG\")[idx].text\n",
    "\n",
    "                c_time = date_reshape(temp_time, now)\n",
    "\n",
    "                like = soup.find_all('button', class_=\"inv-button comment_user-options__aAGUk comment_like__2mSyA\")[idx].text\n",
    "                dislike = soup.find_all('div', class_=\"comment_user-options__aAGUk comment_dislike__1FF6_\")[idx].text\n",
    "\n",
    "                data_temp.append([c_time ,comment, like, dislike])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        df = pd.DataFrame(data_temp, columns=[\"Date\"\n",
    "                                            ,\"Title\"\n",
    "                                            ,\"Like\"\n",
    "                                            ,\"Dislike\"])\n",
    "        df.to_csv(f\"./temp/{str(page).zfill(len(str(pages)))}.csv\", encoding=\"utf-8-sig\")\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    # 분산파일 통합\n",
    "    path = \"./temp\"\n",
    "    file_path = glob.glob(path+\"/*.csv\")\n",
    "\n",
    "    data_temp = []\n",
    "    for f in tqdm(file_path):\n",
    "        temp = os.path.basename(f).split(\".\")[0]\n",
    "        data_temp.extend(pd.read_csv(f, index_col=0).values.tolist())\n",
    "    df = pd.DataFrame(data_temp, columns=[\"Date\"\n",
    "                                        ,\"Title\"\n",
    "                                        ,\"Like\"\n",
    "                                        ,\"Dislike\"])\n",
    "    df.to_csv(f\"./dataset/comment/{name}.csv\", encoding=\"utf-8-sig\")\n",
    "    \n",
    "    # 분산파일 제거\n",
    "    for f in file_path:\n",
    "        os.remove(f)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) instiz crawling\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import time, datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os, sys, glob\n",
    "\n",
    "pages = 500\n",
    "\n",
    "ins_ko_url = \"https://www.instiz.net/name?category=36&k=%5B%EA%B5%AD%EB%82%B4%EC%A3%BC%EC%8B%9D%5D&nolisting=1&stype=9&page=\"\n",
    "ins_us_url = \"https://www.instiz.net/name?category=36&k=%5B%ED%95%B4%EC%99%B8%EC%A3%BC%EC%8B%9D%5D&nolisting=1&stype=9&page=\"\n",
    "\n",
    "def instiz_crawling(name, url, pages):\n",
    "    ################################################################################################\n",
    "    def date_reshape(time):\n",
    "        t = re.sub(r\"[0-9]\",\"\", time)\n",
    "        today = datetime.datetime.today()\n",
    "        if t == \":\":\n",
    "            reshaped_time = today.strftime(\"%Y-%m-%d\")\n",
    "        elif t[0] == \".\":\n",
    "            year = today.year\n",
    "            reshaped_time = str(year) + \"-\" + time.replace(\".\",\"-\")\n",
    "        else:\n",
    "            reshaped_time = time.replace(\".\",\"-\")\n",
    "        return reshaped_time\n",
    "    ################################################################################################\n",
    "\n",
    "    error_pages = []\n",
    "    for page in tqdm(range(1, pages+1)):\n",
    "        url = url + str(page)\n",
    "        try:\n",
    "            res = requests.get(url)\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "            data_temp = []\n",
    "            nrows = 19\n",
    "            for idx in range(nrows):\n",
    "                content = soup.find_all(\"tr\")[idx+20].find_all(\"a\")[0].text.strip()\n",
    "                date = date_reshape(soup.find_all(\"td\", class_=\"listno regdate\")[idx].text.split()[0])\n",
    "                view = soup.find_all(\"td\", class_=\"listno\", width=False, no=False)[2*idx].text\n",
    "                rec = soup.find_all(\"td\", class_=\"listno\", width=False, no=False)[2*idx+1].text\n",
    "                if len(soup.find_all(\"tr\")[idx+20].find_all(\"a\")) == 2:\n",
    "                    reply = soup.find_all(\"tr\")[idx+20].find_all(\"a\")[1].text\n",
    "                else:\n",
    "                    reply = 0\n",
    "                    \n",
    "                data_temp.append([date, content, view, rec, reply])\n",
    "        except:\n",
    "            print(f\"\\tFatal Error: {page} page\")\n",
    "            error_pages.append(page)\n",
    "            \n",
    "        df = pd.DataFrame(data_temp, columns=[\"Date\"\n",
    "                                            ,\"Title\"\n",
    "                                            ,\"View\"\n",
    "                                            ,\"Like\"\n",
    "                                            ,\"Reply\"])\n",
    "        df.to_csv(f\"./instiz_ko/{str(page).zfill(len(str(pages)))}.csv\", encoding=\"utf-8-sig\")\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    # 분산파일 통합\n",
    "    path = \"./temp\"\n",
    "    file_path = glob.glob(path+\"/*.csv\")\n",
    "\n",
    "    data_temp = []\n",
    "    for f in tqdm(file_path):\n",
    "        temp = os.path.basename(f).split(\".\")[0]\n",
    "        data_temp.extend(pd.read_csv(f, index_col=0).values.tolist())\n",
    "    df = pd.DataFrame(data_temp, columns=[\"Date\"\n",
    "                                        ,\"Title\"\n",
    "                                        ,\"View\"\n",
    "                                        ,\"Like\"\n",
    "                                        ,\"Reply\"])\n",
    "    df.to_csv(f\"./dataset/comment/{name}.csv\", encoding=\"utf-8-sig\")\n",
    "    \n",
    "    # 분산파일 제거\n",
    "    for f in file_path:\n",
    "        os.remove(f)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) theqoo crawling\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import time, datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os, sys, glob\n",
    "\n",
    "pages = 500\n",
    "\n",
    "url = \"https://theqoo.net/index.php?mid=stock&page=\"\n",
    "\n",
    "def theqoo_crawling(name, url, pages):\n",
    "    \n",
    "    ################################################################################################\n",
    "    def date_reshape(time):\n",
    "        t = re.sub(r\"[0-9]\",\"\", time)\n",
    "        today = datetime.datetime.today()\n",
    "        if t == \":\":\n",
    "            reshaped_time = today.strftime(\"%Y-%m-%d\")\n",
    "        elif t == \".\":\n",
    "            year = today.year\n",
    "            reshaped_time = str(year) + \"-\" + time.replace(\".\",\"-\")\n",
    "        else:\n",
    "            reshaped_time = time.replace(\".\",\"-\")\n",
    "        return reshaped_time\n",
    "    ################################################################################################\n",
    "\n",
    "    error_pages = []\n",
    "    for page in tqdm(range(1, pages+1)):\n",
    "        url = url + str(page)\n",
    "        try:\n",
    "            res = requests.get(url)\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        except:\n",
    "            print(f\"Error page: {page} page\")\n",
    "            error_pages.append(page)\n",
    "        \n",
    "        nrows = 20\n",
    "        data_temp = []\n",
    "        for idx in range(10, nrows + 10):\n",
    "            title = soup.find_all(\"td\",class_=\"title\")[idx].text.strip()\n",
    "            date = date_reshape(soup.find_all(\"td\",class_=\"time\")[idx].text.strip())\n",
    "            if soup.find_all(\"td\",class_=\"m_no\")[idx].text[-1] == \"만\":\n",
    "                views = float(re.sub(r\"[가-힣]\",\"\",soup.find_all(\"td\",class_=\"m_no\")[idx].text))*10000\n",
    "            else:\n",
    "                views = float(soup.find_all(\"td\",class_=\"m_no\")[idx].text)\n",
    "            if title[-1].isdigit():\n",
    "                reply = int(title[-1])\n",
    "            else:\n",
    "                reply = 0\n",
    "\n",
    "            data_temp.append([date, title, views, reply])\n",
    "        df = pd.DataFrame(data_temp, columns=[\"Date\"\n",
    "                                            ,\"Title\"\n",
    "                                            ,\"View\"\n",
    "                                            ,\"Reply\"])\n",
    "        df[\"Title\"] = df[\"Title\"].apply(lambda x: x[:x.index(\"\\n\")] if \"\\n\" in x else x)\n",
    "        df.to_csv(f\"./temp/{str(page).zfill(len(str(pages)))}.csv\")\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    # 분산파일 통합\n",
    "    path = \"./temp\"\n",
    "    file_path = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "    data_temp = []\n",
    "    for f in tqdm(file_path):\n",
    "        temp = os.path.basename(f).split(\".\")[0]\n",
    "        try:\n",
    "            data_temp.extend(pd.read_csv(f, index_col=0).values.tolist())\n",
    "        except:\n",
    "            data_temp.extend(pd.read_csv(f, index_col=0, encoding=\"cp949\").values.tolist())\n",
    "    df = pd.DataFrame(data_temp, columns=[\"Date\"\n",
    "                                        ,\"Title\"\n",
    "                                        ,\"View\"\n",
    "                                        ,\"Reply\"])\n",
    "    df.to_csv(f\"./dataset/comment/{name}.csv\", encoding=\"utf-8-sig\")\n",
    "    \n",
    "    # 분산파일 제거\n",
    "    for f in file_path:\n",
    "        os.remove(f)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
